\chapter{Related Work}\label{chap:related}

This chapter discusses related work regarding the main contributions of this thesis.
Section~\ref{sec:related-ic} presents previous work on {\itercomp} that consider online scenarios, optimisation across multiple inputs, or related challenges.
In Section~\ref{sec:related-metrics}, we discuss related work that proposes work-based metrics for different purposes.
Finally, Section~\ref{subsec:optimalInstrumentation} presents the main literature on optimal profiling.


\section{{\IterComp}}\label{sec:related-ic}

Until recently, most of the existing work had been focusing on finding the best optimisation through repeated runs using a single input.
Although they demonstrate the potential of {\itercomp}, in real scenarios the user rarely executes the same input dataset multiple times~\citep{bodin98,kisuki99,stephenson03,kulkarni04,agakov06}.
Applying {\itercomp} in light of a single input may not result in a good performance when executing the optimised code with different inputs.

Most of the real world applications are complex enough so that a single input case does not capture the whole range of possible scenarios and program behaviour~\citep{haneda06,fursin07,chen10,chen12a}.
Because programs can exhibit behaviours that differ greatly depending on the input, using a single input for {\itercomp} can produce a poor performance when executed with different inputs.

For this reason, researchers have been studying the impact of using multiple input datasets for performing {\itercomp}.
\cite{chen10,chen12a} evaluate the effectiveness of iterative compilations across a large number of input test cases.
Their main motivation is to answer the question:
\textit{How data input dependent is {\itercomp}?}
When selecting the best optimisation sequence for each input of each program, these optimal optimisation sequences are program-specific and yield average speedups up to 3.75$\times$ over the highest optimisation level of compilers, namely {\flagstype -O3}.
Their results show that, for all the evaluated benchmarks, it is possible to find an optimisation sequence that achieves at least 83\% of the maximum speedup across all input test cases.
In other words, although the best optimisation sequences are both program and input-dependent, it is possible to find a program-specific optimisation sequence that achieves good performance on average.

When optimising a program, the main method for {\itercomp} used by \cite{chen10,chen12a} evaluates each combination of compiler optimisations across all the available inputs, i.e., if $N$ is the number of input test cases and $M$ is the total number of combinations of compiler optimisations, they perform a total of $O(NM)$ runs of the program being optimised.
Furthermore, they use a pre-defined set of only 300 different combinations of compiler optimisations, which represents a very small sample of the optimisation search space for most modern compilers, e.g.
LLVM has 56 distinct optimisation passes and GCC has about 47 high-level (SSA form) optimisation passes and about 25 low-level (RTL) optimisation passes, which in both cases result in much more than $2^{50}$ distinct combinations of compiler optimisations, without considering repetition.

Some recent work~\citep{chen12b,fang15} have applied the same idea of performing input-dependent {\itercomp} to distributed applications on data centres.
In summary, each worker receives a subset of the input dataset, called the evaluation dataset, to perform an \textit{online} {\itercomp} of the code being executed.
Each worker performs the same the method for {\itercomp} used by \cite{chen10,chen12a}, i.e., they evaluate each combination of compiler optimisations across all the evaluation dataset.
However, because the optimisation is performed online, they usually consider a small evaluation dataset and a small number of compiler optimisations.

\cite{fursin07} addressed the problem of comparing the effect of two optimisations on two distinct inputs.
For that purpose, they proposed to use instructions per cycle (IPC) as the metric for performing such comparison.
Their results show that using IPC seems promising as a robust metric for {\itercomp} across large input datasets.
However, some specific optimisation techniques may affect the use of IPC as a robust metric, and especially IPC has been shown to provide poor performance estimation for multi-threaded programs~\citep{alameldeen06,eyerman08}.
In particular, IPC can give a skewed performance measure if threads spend time in \textit{spin-lock loops} or other synchronisation mechanisms. 
Some existing work on performance assessment suggests that total execution time should be used for measuring the performance of multi-threaded programs~\citep{alameldeen06,eyerman08}.
\cite{alameldeen06}, in particular, suggest that a simple work-related metric should be used if the unit of work is representative enough.
Work-related metrics have already been largely used for measuring the performance of throughput-oriented applications, for other applications, however, choosing an appropriate unit of work can be more challenging~\citep{alameldeen06}.

\section{Work and Input Size Metrics}\label{sec:related-metrics}

In the context of computer architecture research, \cite{alameldeen06} suggested work-based metrics as a substitute for IPC.
In particular, they propose the use of both \textit{time per work unit} and \textit{work units per time}.
For example, they suggest a metric based on transactions per minute for throughput-oriented applications.
For iterative scientific applications, they suggest measuring time per iteration.
Choosing an appropriate unit of work is a key aspect of enabling work-based metrics to accurately predict performance~\citep{alameldeen06}. 

In the context of experimental algorithmics, previous work has proposed profiling-based techniques to estimate input sizes~\citep{mcgeoch07,zaparanuks12,coppa14}.
\cite{coppa14}, in particular, propose the concept of \textit{read memory size} for automatically estimating the size of the input passed to a routine, where \textit{read memory size} represents the number of distinct memory cells first accessed by a read operation.
In other words, the \textit{read memory size} metric measures the size of the useful portion of the input's memory footprint.
However, because we are interested in the amount of computational work performed in respect of a given input, the memory footprint of the input may not always have a direct correspondence to  the amount of computational work.

\cite{goldsmith07} use \textit{block frequency} as the measure for performance for empirically describing the asymptotic behaviour of programs, which is known as empirical computational complexity.
Block frequency is a relative metric that represents the number of times a basic block executes~\citep{ball94,ball96}.
They argue in favour of block frequency due to its portability, repeatability and exactness, since it does not suffer from timer resolution problems or non-deterministic noises.
Block frequency also has the advantage of being efficiently profiled by means of automatic code instrumentation~\citep{knuth73,ball94}.

However, in the context of comparing different compiler optimisations, although block frequency would be able to capture aspects of optimisations that simplify the control-flow graph (CFG), measuring work at the basic block resolution would not capture effects of optimisations at the instruction level.
Because of that, in this thesis, we extend the idea of using basic block frequency to measure computational work by also considering the computational cost of each basic block.
The computational cost of a basic block is given by weighting the instructions that it contains.

\section{Program Profiling} \label{subsec:optimalInstrumentation}

Program profiling concerns with the acquisition of dynamic information collected during the program's execution.
Since early work, program profiling has been used mainly to find performance bottlenecks and to focus optimisations.
\cite{knuth71} introduces program profiling as a table of frequency counts that records execution frequency of each statement during a typical run of a program.
%\cite{knuth71} profiles the execution frequency of each FORTRAN statement during the execution of representative programs.
Since then, program profiling has been widely used in optimising compilers.
For example, most of the industrial-strength compilers provide block frequency profiling, where it records the execution frequency of each basic block in the program.

In order to profile block frequency, the program can be instrumented with counters that determine how many times each basic block in a program executes.
A naive instrumentation would consist basically of having a counter for each basic block which is incremented every time the basic block is reached.
Although the naive instrumentation was commonly used in practice~\citep{knuth71}, it is a very invasive instrumentation that imposes an unnecessarily high overhead in the instrumented program.
An optimal instrumentation based on the principle of \textit{conservation of flow} (\textit{Kirchhoff's first law}\footnote{Gustav Kirchhoff defined two equalities about electric circuits, known as Kirchhoff's circuit laws. The first one is about current and the second about potential difference.}) have been originally proposed by \cite{nahapetian73} and \cite{knuth73}.
While \cite{knuth73} proposed an optimal solution for basic block profiling with \textit{vertex counters}, \cite{ball94} showed that an optimal basic block profiling with \textit{edge counters} provides the best instrumentation for block-frequency profiling.
Further overhead reduction of the optimal instrumentation was later proposed by placing the counters in edges that are less likely to be executed~\cite{forman81,ball94}.

\begin{definition}[Kirchhoff's first law]
The amount of flow into a vertex equals the amount of output flow, i.e. the sum of the incoming edges of a vertex equals the sum of outgoing edges of the same vertex.
\end{definition}

The optimal instrumentation places probes in edges as the basic block frequency can be derived by summing either the flow of the incoming or outgoing edges.
However, it uses the Kirchhoff's first law in order to place probes in a subset of the edges that allows to later infer the flow of all edges.
Previous work has shown that a set of edges represents the minimum number of probes for profiling block frequency if and only if the complementary set of edges forms a spanning tree~\citep{nahapetian73,ball94}.
In other words, after determining a spanning tree of the CFG, probes need to be placed only in the edges from the complement of a spanning tree, usually called \textit{cotree}.
Because the edge frequencies satisfy Kirchhoff's first law, each edge flow can be uniquely determined as an algebraic sum of the known edge flows from the cotree~\citep{nahapetian73,ball94}.

\begin{lstlisting}[caption={Post-processing of the CFG for populating all edge flows based on the collected probes.}, label={lst:populateEdgeFlows}]
// Inputs: CFG with the known edges flows from
//         the cotree (collected probes).
// Output: Updated CFG with all edge flows.
populateEdgeFlows(G) {
  changed = true
  while changed:
    changed = false
    for B in G.vertices():
      unIN = count( G.unknownIncomingEdges(B) )
      unOUT = count( G.unknownOutgoingEdges(B) )
      if unIN==0 and unOUT==1:
        //sum known incoming and outgoing edges in B
        sIN = sum( G.incomingEdges(B) )
        sOUT = sum( G.outgoingEdges(B) )
        //update unknown outgoing edge in B with (sIN-sOUT)
        G.setUnknownOutgoingEdge(B, (sIN-sOUT))
        changed = true
      if unIN==1 and unOUT==0:
        //sum known incoming and outgoing edges in B
        sIN = sum( G.incomingEdges(B) )
        sOUT = sum( G.outgoingEdges(B) )
        //update unknown incoming edge in B with (sOUT-sIN)
        G.setUnknownIncomingEdge(B, (sOUT-sIN))
        changed = true
}
\end{lstlisting}

The optimal block-frequency instrumentation happens in two main stages:
\textit{(i.) Before execution:} The code is instrumented with the edge counters, i.e., it requires one global counter for each edge selected to contain a probe.
\textit{(ii.) After execution:} The information from the recorded probes is propagated in the CFGs of the program.
\lstlistingname~\ref{lst:populateEdgeFlows} shows the algorithm for the post-processing of a CFG, which requires the profiling information collected by the probes.

\lstlistingname~\ref{lst:populateEdgeFlows} is guaranteed to terminate because the probed edge flows on the complement of a spanning tree are necessary and sufficient to compute all edge flows~\citep{nahapetian73,forman81}.
Intuitively, if all the edge flows are known for the complement of a spanning tree then at any leaf of the spanning tree there is only one unknown edge flow.
This unknown edge flow can be calculated by Kirchhoff's first law.
This process repeats until all the unknown edge flows have been calculated.
Although this instrumentation algorithm is proved to produce the optimal placement of probes for well-structured CFGs, it may produce a sub-optimal placement for some unstructured CFGs~\citep{ball94}.

This briefly described proof suggests that the edges can be more efficiently populated by a bottom-up propagation in the spanning tree.
By performing a post-order traversal of the spanning tree, i.e. starting from the leaves, we can then apply the flow equation from the Kirchhoff's first law.
At each node of the spanning tree, we first sum the known incoming and outgoing edges, and then the unknown edge flow will by computed by subtracting the minimum of the two sums from the maximum (as before).
This bottom-up propagation allows to populate the edge flows in a single pass over the basic blocks.

\cite{forman81} and \cite{ball94} propose to optimise the placement of the probes with respect to edges that are less likely to be executed.
It works by considering a weighting that assigns a non-negative value to each edge in the CFG.
The overhead cost of profiling a set of edges is considered to be proportional to the sum of the weights of the edges.
These weights can be obtained either by empirical measurements from previous executions or by static estimates at compile-time.
In order to minimise the profiling overhead, the instrumentation computes the maximum spanning tree to avoid probing in frequently executed edges.

\lstlistingname~\ref{lst:instrumentCFG} presents the algorithm for the optimal placement of probes.
If present, it uses edge-frequency profiling from previous executions,
otherwise, it uses static estimates of edge frequencies.
Afterwards, it makes use of the edge frequencies for computing the maximum spanning tree.
LLVM implements Kruskal's algorithm using the union-find data structure~(see \lstlistingname~\ref{lst:kruskalMST})
for computing the maximum spanning tree of the CFG.
Only edges in the cotree are instrumented.

%\newpage 
\begin{lstlisting}[caption={Optimal placement of probes for block frequency.}, label={lst:instrumentCFG}]
// Input: CFG	
instrumentCFG(G) {
  if not hasEdgeFrequency(G):
    estimateEdgeFrequency(G)
  T = MaxSpanningTree(G)
  for e in G.edges()-T.edges():
    placeProbe(e)
}
\end{lstlisting}

Because edges in a CFG are abstractions for a transition of control flow,
the actual code for the probes needs to be placed in one of the endpoints of instrumented edges.
\lstlistingname~\ref{lst:placeProbe} selects which basic block of an edge will be instrumented.
\textit{Critical edges} are split, inserting a new basic block between its endpoints.
Critical edges are edges from a basic block that contains multiple successors to a basic block that contains multiple predecessors.

\begin{lstlisting}[caption={For a given edge, this procedure selects which basic block to place the instrumented code.
                            If the edge is critical, an intermediate basic block is created for the instrumentation.}, label={lst:placeProbe}]
// Input: edge selected for instrumentation
placeProbe(e){
  (B1, B2) = e
  if B1 is virtual:
    insertProbe(B2)
  else if B2 is virtual:
    insertProbe(B1)
  else if countSuccessors(B1)<=1:
    insertProbe(B1)
  else if e is not critical:
    insertProbe(B2)
  else
    B = splitCriticalEdge(e)
    insertProbe(B)
}
\end{lstlisting}

Notice that when instrumentation is performed guided by empirical measurements from previous executions, it means that edge-profiling information is used in order to produce a better edge-profiling instrumentation.
The next section presents the main algorithms for producing static estimates for the weights of the edges in a CFG.

\subsection{Static Estimates of Edge Frequencies}

\cite{ball93} presented a simple algorithm that predicts the outcome of conditional branches with a reasonably good accuracy.
For this purpose, they used several branch heuristics that were derived by measuring, on a large number of programs, the probability of branches being taken in respect of some \textit{ad-hoc} features from the programs.
Their algorithm selects, for each branch, the first heuristic that applies to the branch, in a given priority order of the heuristics.
The \textit{ad-hoc} heuristics defined by \cite{ball93} are:

\noindent\textbf{Loop branch heuristic (probability 88\%):} Probability of an edge back to a loop's head being executed.

\noindent\textbf{Loop exit heuristic (probability 80\%):} Probability that a comparison inside a loop will \textit{not} exit the loop. This heuristic does not apply to latch blocks, i.e. basic blocks that contain a branch back to the header of the loop.

\noindent\textbf{Pointer heuristic (probability 60\%):} Probability that a comparison between two pointers, where one of them can be a null pointer, will fail.

\noindent\textbf{Opcode heuristic (probability 84\%):} Probability that a comparison of an integer being less than zero, less than or equal to zero, or equal to a constant will fail.

\noindent\textbf{Guard heuristic (probability 62\%):} For a comparison with a register as operand, where the register is defined in a successor basic block which is not a post-dominator. The probability that the successor basic block is reached.

\noindent\textbf{Loop header heuristic (probability 75\%):} Probability of reaching a successor block that is a loop header (or pre-header) but not a post-dominator.

\noindent\textbf{Call heuristic (probability 78\%):} Probability of reaching a successor block that is not a post-dominator but contains a function call.

\noindent\textbf{Store heuristic (probability 55\%):} Probability of reaching a successor block that is not a post-dominator but contains a store instruction.

\noindent\textbf{Return heuristic (probability 72\%):} Probability of reaching a successor block that contains a return instruction.


\cite{wu94} proposed an algorithm for statically estimating edge frequencies, which improves on the work of \cite{wagner94} and \cite{ball93}.
This algorithm is able to combine several heuristics of the outcome of a branch into an estimated probability of the branch being taken.
\cite{wu94} use the \textit{ad-hoc} heuristics defined by \cite{ball93} as their initial predictions.
They also use the Dempster-Shafer theory~\citep{shafer76} that provides the necessary mathematical technique for combining evidence from the different heuristics in order to produce more accurate estimates.
These branch probabilities can then be used to estimate execution frequencies for all the edges in a CFG.

