\chapter{Conclusions}\label{chap:conclusion}

In this work, our main goal is to enable {\itercomp} in online scenarios.
We define the online scenario as having the restriction that programs execute multiple inputs and distinct inputs are executed only once.
To this end, we propose the use of a work-based performance metric for guiding {\itercomp},
where a work profiling is used to measure the amount of work for each execution of the program.
Because having a low overhead instrumentation is essential in this online scenario, we propose two relaxation algorithms which provide a trade-off between measurement accuracy and overhead.
The proposed whole program relaxation was able to achieve an average overhead of about 5\% over the non-instrumented program with the {\flagstype -O3} optimisation.
This overhead represents a $2.1\times$ reduction over the optimal work profiling, while incurring in a dynamic measurement error of less than 3\%.

Our results corroborate the use of the work-based performance metric for guiding online {\itercomp}.
Under real conditions, it was able to achieve good performance improvements when compared to the oracles, which were allowed to execute the program with the same input multiple times,
achieving an average improvement of 5.4\% and a maximum of about 20\%.
Contrary to what previous work has suggested, we have also shown that instructions per cycle (IPC) is not a good metric for guiding online {\itercomp}.

\section{Future Work}

%\textbf{Elaborate}Create a context-aware instrumentation in order to avoid updates to the global counter inside function calls in tight loops, for which function cloning could be employed.
As future work one could improve the work instrumentation by developing a context-aware instrumentation to avoid updates to the global counter in function calls inside tight loops.
Function cloning could be employed to address this issue.
Updates to the global counter could be avoided by exchanging the local counter with the specialised version of cloned functions via argument and return values.
This transformation creates opportunities for future optimisations, e.g., after applying function inlining, while updates to global variables tend to prevent optimisations.

%\textbf{Elaborate}Merge probes in branches with similar work value.
Regarding the relaxed instrumentation,
one could merge probes in branching paths for which the difference between the amount of work computed in each path stays within a given threshold.
In this case, the branch could be considered as a straight-line code for which the amount of work is a weighted average, based on the branching probability, of the work for each path.
Another improvement to the relaxed instrumentation would be to perform a loop-aware relaxation.
In other words, loops for which the trip count has a known upper bound could be considered as if it was unrolled when performing relaxation on the outer scope.
Hence the whole region could be perceived as a single DAG.

For multi-threaded programs, one could analyse the trade-offs of performing a synchronised update to the global work counter versus allowing race conditions.
Although this trade-off can produce non-deterministic errors in the measurement of the amount of work, in most programs, this error might be non-relevant compared to the performance benefit of avoiding synchronisation.

A mechanism similar to the one used for the online {\itercomp} with the work-based performance (WP) metric can also be applied to perform
runtime auto-tuning or multi-versioning optimisations.
This mechanism would be particularly useful for irregular programs.
In the case of multi-versioning optimisations, one could have multiple instrumented and non-instrumented versions of hop loops or functions.
Initially, the instrumented version would be executed to collect the WP metric and then be able to select which optimised version to use in future executions, 
for which the non-instrumented version would be used.

In the area of experimental algorithmics, work-based metrics are largely used in order to empirically estimate the asymptotical complexity of programs.
These estimates usually comprise measuring the input size and the computational cost during the execution of a program.
To that end, the work instrumentation, with the proposed relaxation algorithms, could be adapted to these other work-based metrics in order to provide low-overhead algorithmic profiling.
