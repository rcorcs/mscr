
\chapter{Introduction}

Modern optimising compilers have reached a high level of sophistication, providing a large number of optimisations.
Because of the unpredictability of optimisation interactions, in addition to the growing complexity of processor architectures and applications, the correct choice of optimisations and their ordering can have a significant impact on the performance of the code being optimised~\citep{pan06,fursin07,kulkarni12,purini13}.

Each of these optimisation passes interacts with the code in complicated ways, depending on all other optimisations and the order they were applied to the code being optimised.
Understanding the interactions of optimisations is very important in determining a good solution to the phase-ordering problem~\citep{touati06,kulkarni12}.
Because of all those dependencies and interactions, although most compiler optimisations yield significant improvements in many programs, the potential for performance degradation in certain program patterns is known to compiler writers and many users~\citep{pan06,zhou12,kulkarni12}.
%Traditional compilers typically apply the same set of optimisation in a fixed order to all functions in a program, without regard the code being optimised.

Compiler writers typically use a combination of experience, insight, and experimentation on benchmark programs to construct a set of default optimisation sequences,
which are commonly associated with the optimisation options like {\flagstype -O1}, {\flagstype -O2}, {\flagstype -O3}, {\flagstype -Os}, and {\flagstype -Oz}.
Although these default optimisation sequences are expected to provide good performance on average, they are often sub-optimal for individual programs.
Moreover, they usually do not include all the available optimisation passes, and they are always applied in the same pre-defined order, without regard to the program being optimised~\citep{pan06,cavazos07,zhou12,kulkarni12}.
Another major drawback concerns the fact that these default optimisation sequences should be reconstructed whenever the backend of the compiler re-targeted to a new processor architecture or new analysis and optimisations are implemented in the compiler.
%in hopes that a programmer will know which combination of optimisations will benefit their code~\cite{pan06,cavazos07,zhou12,kulkarni12}.

Efficiently selecting the best optimisation sequence for a given program or program section remains an open problem.
A well-known compilation technique that addresses this challenge is {\itercomp}.
{\Itercomp} has the ability to adapt to new platforms, program and workload while still having a systematic and simple optimisation process.
It works by repeatedly evaluating a large number of compiler optimisations until the best combination is found for a particular program~\citep{fursin07,chen10}.
The main challenge concerning {\itercomp} is the need for efficiently exploring such a large optimisation space~\citep{fursin07,cavazos07,zhou12}.

Until recently, most of the existing work had been focusing on finding the best optimisation through repeated runs using a single input.
Although they demonstrate the potential of {\itercomp}, in real scenarios the user rarely executes the same input dataset multiple times~\citep{bodin98,kisuki99,stephenson03,kulkarni04,agakov06}.
Applying {\itercomp} in light of a single input may not result in a good performance when executing the optimised code with different inputs.

Most of the real-world applications are complex enough so that a single input case does not capture the whole range of possible scenarios and program behaviour~\citep{haneda06,fursin07,chen10,chen12a}.
Because programs can exhibit behaviours that differ greatly depending on the input,
%This is something that is well known when writing test cases for correctness.
%We can extend the same argument when optimising the code for performance.
using a single input for {\itercomp} can produce a poor performance when executed with different inputs.

Recent work has been studying the impact of using multiple input datasets for performing {\itercomp}.
This previous work suggests that optimising based on a representative range of input datasets allows for selecting a robust compiler optimisation that produces a good performance in real scenarios where the input is expected to change constantly~\citep{haneda06,fursin07,chen10,chen12a,chen12b,fang15,mpeis16}.
Their results show that a limited number of input datasets may be sufficient to obtain a well-optimised program for a wider range of different inputs~\citep{haneda06,fursin07,chen10,chen12a}.
Finding such a robust combination of compiler optimisations by means of {\itercomp} across a large range of inputs may be very time-consuming.
%In order to speed up this process we intend to reduce the number of actual executions during the exploration of the optimisation space without much degradation of the final selected optimisation.

\section{{\IterComp} in Online Scenarios}

In this work, our main goal is to enable {\itercomp} in \textit{online} scenarios.
For the purposes of this thesis, we define the online scenario as having the restriction that programs execute multiple inputs and distinct inputs are executed only once.
This online aspect is usually found in mobile and data centre platforms~\citep{chen12b,fang15,mpeis16}, where the goal is to optimise programs based on the workload of a particular user (or group of users) between executions.
Online {\itercomp} must target optimal performance across different inputs.

Because of the restriction of having a single execution per input, it is not possible to measure speedup for comparing optimisations.
On the other hand, measuring just execution time, for example, is useful only if the amount of work is constant between executions with different inputs.
While previous work have suggested using \textit{instructions per cycle} (IPC) for performing {\itercomp} in \textit{online} scenarios, IPC have no correlation with speedup~\citep{fursin07}.

In order to address this challenge, we propose the use of a work-based metric to compare the performance of different optimisations across multiple executions of the program with distinct inputs.
We instrument the program for measuring the amount of work it performed during its execution.
Having a low overhead instrumentation is essential in this online scenario for two main reasons:
$(i)$ the user is directly affected by large overheads;
$(ii)$ a highly intrusive instrumentation can have a significant impact on the effect of the optimisations.
With the purpose of reducing profiling overhead, we propose two relaxation algorithms which provide a trade-off between measurement accuracy and overhead.
The first is a relaxation algorithm that operates on the level of regions of functions, while the second performs the relaxation considering the whole program at the same time.

Our experimental evaluation shows that performing online {\itercomp} guided by the work-based performance (WP) metric good results compared to the oracle, which is allowed to execute each input multiple times in order to use the actual speedup for guiding the {\itercomp}.
Online {\itercomp} guided by the WP metric is able to achieve an average of 7.5\% and a maximum of 33\% improvement over the standard {\flagstype -O3} optimisation.
Moreover, the experiments regarding the work profiling show that both relaxation algorithms are able to significantly reduce the profiling overhead while incurring a dynamic error of less than 5\% in the work measurement.
The whole program relaxation achieves an average of $2\times$ reduction in the overhead compared with the optimal profiling technique, while the more conservative relaxation that operates per region achieves an average improvement of 40\% over the optimal profiling.

\section{Contributions}

%Our main contributions are the following:
%\begin{itemize}
%\item The use of a work-based performance metric in order to enable \textit{online} {\itercomp} by comparing different combination of compiler optimisations even when executed with distinct inputs.
%\item We propose a relaxed instrumentation for low overhead profiling, with a controlled trade-off between accuracy and overhead.
%\end{itemize}

To summarise, the main contributions of this thesis are the following:
\begin{itemize}
\item We propose the use of a work-based performance (WP) metric for comparing different optimised versions of a program.
      In particular, we use the WP metric to guide {\itercomp} in an online scenario, where the program is expected to execute only once for distinct inputs.
       
\item Contrary to what previous work has suggested, we show that instructions per cycle (IPC) is not a good metric for online {\itercomp}.
\item We adapt the block frequency profiling in order to measure the WP metric.
\item A relaxation algorithm is proposed for lowering the overhead of the work profiling, with a controlled trade-off between accuracy and overhead.
\item A whole program relaxation is proposed in order to further reduce the overhead of the work profiling.
\end{itemize}

\section{Thesis Outline}

The remaining of this thesis is structured as follows.
Chapter~\ref{chap:background} provides background information with basic concepts and theory used in this thesis
Chapter~\ref{chap:related} presents related work regarding {\itercomp} using multiple inputs or applied in online scenarios,
profiling of work-based metrics or input size, and algorithms related to optimal profiling.
Our main contributions are divided into two chapters.
First, Chapter~\ref{chap:oic} presents both the work-based performance metric and the architecture for online {\itercomp}.
Second, Chapter~\ref{chap:instr} describes the work profiling and techniques for reducing its overheads.
An experimental evaluation is provided in Chapter~\ref{chap:eval} with our final remarks in Chapter~\ref{chap:conclusion}.
